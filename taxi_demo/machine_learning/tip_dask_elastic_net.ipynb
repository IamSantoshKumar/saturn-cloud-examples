{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Dask\n",
    "\n",
    "**Hardware**: 10 nodes - r5.8xlarge's (32 CPU, 256 GB RAM each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import MLUtils\n",
    "\n",
    "ml_utils = MLUtils(\n",
    "    ml_task='tip',\n",
    "    tool='dask',\n",
    "    model='elastic_net',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-07 14:52:25] INFO - dask-saturn | Cluster is ready\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36585aeb2a5d4ccc97eb26f33495c209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SaturnCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "cluster = SaturnCluster(n_workers=10, scheduler_size='xlarge', worker_size='8xlarge', nthreads=32)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.8 ms, sys: 8.2 ms, total: 78 ms\n",
      "Wall time: 2.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10994913"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tip_train = dd.read_parquet(f'{ml_utils.taxi_path}/data/ml/tip_train_sample', engine='pyarrow')\n",
    "len(tip_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's take the same sample we used in the single node scikit example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tip_train.sample(frac=0.1, replace=False, random_state=42)\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search\n",
    "\n",
    "- use `dask-ml` preprocessing and grid search classes\n",
    "- still using `sklearn.linear_model.ElasticNet` for model fitting\n",
    "- we won't `refit` with best model, because we want to use `dask_ml.wrappers.ParallelPostFit` to use dask to parallelize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from dask_ml.preprocessing import StandardScaler, DummyEncoder, Categorizer\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "features = ml_utils.tip_vars.features\n",
    "y_col = ml_utils.tip_vars.y_col\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('categorize', Categorizer(columns=ml_utils.tip_vars.categorical_feat)),\n",
    "    ('onehot', DummyEncoder(columns=ml_utils.tip_vars.categorical_feat)),\n",
    "    ('scale', ColumnTransformer(transformers=[('num', StandardScaler(), ml_utils.tip_vars.numeric_feat)])),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100)),\n",
    "])\n",
    "\n",
    "params = ml_utils.tip_vars.elastic_net_grid_search_params\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=3, scoring='neg_mean_squared_error', refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('score-58dd25866dd2ba6aa2f5b167c675e31e', 39, 2) has failed... retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 85.2 ms, total: 1.14 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(sample[features], sample[y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best_params manually because we set `refit=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0, 'clf__l1_ratio': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = (pd.DataFrame(grid_search.cv_results_)\n",
    "               .sort_values('mean_test_score', ascending=False)\n",
    "               .loc[0, 'params'])\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This wrapper allows us to parallelize predictions using Dask. The `fit` step is not affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 384 ms, sys: 233 ms, total: 617 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "best_estimator = ParallelPostFit(estimator=pipeline)\n",
    "_ = best_estimator.fit(sample[features], sample[y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Grab the sklearn estimator out of the dask wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator = best_estimator.estimator.named_steps['clf']\n",
    "ml_utils.write_model(sklearn_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set\n",
    "\n",
    "And calculate metrics. Save predictions and metrics to S3.\n",
    "\n",
    "Notice that the below cell runs super fast, because it hasn't actually done anything due to Dask's lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 4.56 ms, total: 123 ms\n",
      "Wall time: 644 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "amt_test = dd.read_parquet(f'{ml_utils.taxi_path}/data/ml/tip_test', engine='pyarrow')\n",
    "preds = amt_test[['id', y_col]].copy()\n",
    "preds.columns = ['id', 'actual']\n",
    "preds = preds.assign(predicted=best_estimator.predict(amt_test[features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can `persist` the DataFrame to compute all the predictions and store in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 ms, sys: 7.93 ms, total: 75.9 ms\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e8109754e3e4cb7879c4e9ee216d58d</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a30e7c87866f417ab15dee5617f272a0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a7a611d0809489d99a5120727e0476a</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736e84ca12a640cc858c210bd58f744c</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f2c24299d9a34ce986b7a271c5cc80b2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id    actual  predicted\n",
       "0  8e8109754e3e4cb7879c4e9ee216d58d  0.097087   0.153217\n",
       "1  a30e7c87866f417ab15dee5617f272a0  0.166667   0.153217\n",
       "2  1a7a611d0809489d99a5120727e0476a  0.120000   0.153217\n",
       "3  736e84ca12a640cc858c210bd58f744c  0.089474   0.153217\n",
       "4  f2c24299d9a34ce986b7a271c5cc80b2  0.000000   0.153217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from dask.distributed import wait\n",
    "preds = preds.persist()\n",
    "_ = wait(preds)\n",
    "\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.2 ms, sys: 6.66 ms, total: 86.8 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_utils.write_predictions(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If the `preds` DataFrame was _really_ big, you would want to use `dask_ml.metrics.mean_squared_error`. Here, the `preds` columns are pulled down to the client because we're using `sklearn.metrics.mean_squared_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 598 ms, sys: 982 ms, total: 1.58 s\n",
      "Wall time: 2.51 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ml_task</th>\n",
       "      <th>tool</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tip</td>\n",
       "      <td>dask</td>\n",
       "      <td>elastic_net</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.052227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ml_task  tool        model metric     value\n",
       "0     tip  dask  elastic_net   rmse  0.052227"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = mean_squared_error(preds.actual, preds.predicted, squared=False)\n",
    "ml_utils.write_metric_df('rmse', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
