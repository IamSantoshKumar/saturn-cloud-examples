{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import hvplot.dask, hvplot.pandas\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "import time\n",
    "\n",
    "cluster = SaturnCluster(n_workers=10, scheduler_size='xlarge', worker_size='8xlarge', nthreads=32)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TAXI_S3' not in os.environ:\n",
    "    raise ValueError('Set TAXI_S3 environment variable to an S3 location that you have read/write access to')\n",
    "taxi_path = os.environ['TAXI_S3']\n",
    "data_path = f\"{taxi_path}/data/taxi_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = dd.read_parquet(data_path, engine='pyarrow', assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to spatial join to pull in older data\n",
    "ddf = taxi[(taxi.pickup_taxizone_id.notnull() & taxi.dropoff_taxizone_id.notnull()) &\n",
    "           (taxi.pickup_datetime >= datetime.datetime(2017, 1, 1)) & \n",
    "           (taxi.pickup_datetime < datetime.datetime(2020, 1, 1))]\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: downsample\n",
    "\n",
    "To run this notebook quickly on a smaller cluster you can downsample and persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = ddf.sample(frac=0.01).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "\n",
    "We'll distill some features out of the datetime component of the data. This is similar to the feature engineering that is done in other places in this demo, but we'll only create the features that'll be most useful in the visuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"pickup_hour\"] = ddf.pickup_datetime.dt.hour\n",
    "ddf[\"dropoff_hour\"] = ddf.dropoff_datetime.dt.hour\n",
    "ddf[\"pickup_weekday\"] = ddf.pickup_datetime.dt.weekday\n",
    "ddf[\"dropoff_weekday\"] = ddf.dropoff_datetime.dt.weekday\n",
    "ddf[\"percent_tip\"] = (ddf[\"tip_amount\"] / ddf[\"fare_amount\"]).replace([np.inf, -np.inf], np.nan) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take out the extreme high values since they disrupt the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"percent_tip\"] = ddf[\"percent_tip\"].apply(lambda x: np.nan if x > 1000 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is now small enough to fit in worker memory, we'll drop any unneeded columns and repartition and persist for easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf.drop([\n",
    "    'vendor_id', 'rate_code_id', 'store_and_fwd_flag','pickup_latitude',\n",
    "    'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'extra', 'mta_tax',\n",
    "    'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge'\n",
    "], axis=1)\n",
    "ddf = ddf.repartition(npartitions=80).persist()\n",
    "_ = wait(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets\n",
    "\n",
    "We'll resample to an hourly timestep so that we don't have to pass around so much data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_ddf = ddf[[\"pickup_datetime\", \"percent_tip\"]].set_index(\"pickup_datetime\").dropna()\n",
    "tips = tip_ddf.resample('1H').mean().compute()\n",
    "\n",
    "tips.to_csv(\"./data/pickup_average_percent_tip_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_ddf = ddf[[\"pickup_datetime\", \"fare_amount\"]].set_index(\"pickup_datetime\").dropna()\n",
    "fare = fare_ddf.resample('1H').mean().compute()\n",
    "\n",
    "fare.to_csv(\"./data/pickup_average_fare_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate datasets\n",
    "\n",
    "Since our data is rather large and will mostly be viewed in grouped aggregates, we can do some aggregation now and save it off for use in plots later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (ddf\n",
    "            .groupby([\n",
    "                f\"{value}_taxizone_id\", \n",
    "                f\"{value}_hour\",  \n",
    "                f\"{value}_weekday\",\n",
    "            ])\n",
    "            .agg({\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            })\n",
    "            .compute()\n",
    "           )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename({\n",
    "        (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "        (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "        (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "        (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "        (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        \n",
    "    }, axis=1).reset_index(level=[1, 2])\n",
    "    data.to_csv(f\"data/{value}_grouped_by_zone_and_time.csv\")\n",
    "\n",
    "grouped_zone_and_time = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (ddf\n",
    "            .groupby([\n",
    "                f\"{value}_taxizone_id\", \n",
    "            ])\n",
    "            .agg({\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            })\n",
    "            .compute()\n",
    "           )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename({\n",
    "        (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "        (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "        (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "        (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "        (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        \n",
    "    }, axis=1)\n",
    "    data.to_csv(f\"data/{value}_grouped_by_zone.csv\")\n",
    "\n",
    "grouped_zone = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"pickup\"\n",
    "data = (ddf\n",
    "        .groupby([\n",
    "            f\"{value}_hour\", \n",
    "            f\"{value}_weekday\"\n",
    "        ])\n",
    "        .agg({\n",
    "            \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "            \"trip_distance\": [\"mean\"],\n",
    "            \"percent_tip\": [\"mean\"],\n",
    "        })\n",
    "        .compute()\n",
    "       )\n",
    "data.columns = data.columns.to_flat_index()\n",
    "data = data.rename({\n",
    "    (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "    (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "    (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "    (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "    (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "\n",
    "}, axis=1)\n",
    "\n",
    "data.to_csv(f\"data/{value}_grouped_by_time.csv\")\n",
    "grouped_time = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ddf = dd.read_parquet(f\"{taxi_path}/data/ml/tip_test\", engine=\"pyarrow\")\n",
    "test_ddf = test_ddf.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_timeseries = test_ddf[[\"pickup_datetime\", \"tip_fraction\"]].repartition(npartitions=10).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "prediction_files = fs.glob(f\"{taxi_path}/ml_results/predictions/tip*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for f in prediction_files:\n",
    "    name = os.path.basename(f)\n",
    "    prediction = dd.read_parquet(f\"s3://{f}\", engine=\"pyarrow\")\n",
    "    if \"predicted\" in prediction.columns:\n",
    "        print(f\"Joining with {name}\")\n",
    "        prediction = prediction.set_index(\"id\").repartition(npartitions=10)\n",
    "        prediction_timeseries = prediction_timeseries.join(prediction[[\"predicted\"]]).rename(columns={\"predicted\": name})\n",
    "\n",
    "ml_ddf = prediction_timeseries.set_index(\"pickup_datetime\")\n",
    "ml_df = ml_ddf.resample(\"1H\").mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df.to_csv(\"./data/ml_prediction_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shape files for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with fs.open('s3://nyc-tlc/misc/taxi_zones.zip') as f:\n",
    "    with zipfile.ZipFile(f) as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.put('data', f'{taxi_path}/data/dashboard', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "To make use of the new datasets we can visualize all the data at once using a grouped heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zone_and_time.hvplot.heatmap(\n",
    "    x=\"dropoff_weekday\", \n",
    "    y=\"dropoff_hour\", \n",
    "    C=\"average_percent_tip\",\n",
    "    groupby=\"dropoff_taxizone_id\", \n",
    "    responsive=True, min_height=600, cmap=\"viridis\", clim=(0, 20),\n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset that is only grouped by zone can be paired with other information such as geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "zones = gpd.read_file('./data/taxi_zones.shp').to_crs('epsg:4326')\n",
    "joined = zones.join(grouped_zone, on=\"LocationID\")\n",
    "\n",
    "joined.hvplot(x=\"longitude\", y=\"latitude\", c=\"average_fare\", \n",
    "              geo=True, tiles=\"CartoLight\", cmap=\"fire\", alpha=0.5,\n",
    "              hover_cols=[\"zone\", \"borough\"], \n",
    "              title=\"Average fare by dropoff location\",\n",
    "              height=600, width=800, clim=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payment Type Pie Chart\n",
    "\n",
    "Other vizualisations can be contructed straight from the raw data and saved for embedding in the dashboard later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_type = ddf.payment_type.value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = payment_type.index.map({\n",
    "    1: \"Credit card\", \n",
    "    2: \"Cash\", \n",
    "    3: \"No charge\", \n",
    "    4: \"Dispute\", \n",
    "    5: \"Unknown\",\n",
    "}).astype(\"category\")\n",
    "\n",
    "payment_type.index = new_index\n",
    "payment_type.name = \"value\"\n",
    "payment_type.index.name = \"payment_type\"\n",
    "payment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import cumsum\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "data = payment_type.reset_index()\n",
    "data['angle'] = data['value']/data['value'].sum() * 2*pi\n",
    "data[\"label\"] = data.value.apply(lambda x: f\"{x/1e6: .0f} M\")\n",
    "data[\"frac\"] = data.angle.apply(lambda x: f\"{x / (2*pi): .0%}\")\n",
    "\n",
    "data = data[:2]\n",
    "data['color'] = [\"thistle\", \"lightblue\"]\n",
    "\n",
    "\n",
    "p = figure(plot_height=350, plot_width=350, toolbar_location=None,\n",
    "           x_range=(-.5, .5), y_range=(0, 2), title=\"Payment Type\")\n",
    "\n",
    "p.wedge(x=0, y=1, radius=0.4,\n",
    "        start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "        line_color=\"white\", fill_color='color', source=data)\n",
    "\n",
    "p.text(x=[-0.2, 0.07], y=[1.4, 0.7], text=data[\"payment_type\"].astype(str) + \":\\n  \" + data[\"label\"] + \"\\n  \" + data[\"frac\"],\n",
    "       text_align=\"left\", text_baseline=\"top\", text_font_size=\"15px\")\n",
    "\n",
    "\n",
    "p.title.text_font_size = \"20px\"\n",
    "p.axis.axis_label=None\n",
    "p.axis.visible=False\n",
    "p.grid.grid_line_color = None\n",
    "p.outline_line_width = 0\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this doesn't work on Saturn at this time.\n",
    "\n",
    "from bokeh.io import export_svgs\n",
    "\n",
    "p.output_backend = \"svg\"\n",
    "export_svgs(p, filename=\"pie_chart.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
